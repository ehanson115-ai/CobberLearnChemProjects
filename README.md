# CobberLearnChemProjects
Projects for Foundations of Machine Learning for Chemistry.  

AlkaneBoilingPoints: This simple code plots the number of carbon atoms in the alkane on the x-axis and the boiling points of that alkane on the y-axis. It generates a properly labeled graph using Matplotlib.  
    
PubChemFetcher: This Python script uses PubChemPy to fetch and display key chemical properties for any compound by name. It retrieves the following descriptors: Molecular Formula, Molecular Weight, SMILES string, Topological Polar Surface Area (TPSA), and Heavy Atom Count. The output is formatted neatly for easy reading. The script is reusable for any compound, and can be run interactively to analyze multiple compounds without editing the code.  
  
MoleculeExplorer: his Python script allows users to input one or more SMILES strings and calculates key chemical properties for each molecule using RDKit. Properties include: Exact Molecular Weight, Hydrogen Bond Donors, Topological Polar Surface Area (TPSA), Total number of atoms, Number of bonds, Number of rings, Number of rotatable bonds, The program outputs results in a clean, easy-to-read table and supports analyzing multiple molecules easily from a SMILES string and molecules name inputted by a user.  

MakingDataWhole: This script loads the Titanic dataset, handles missing ages using mean imputation, manual KNN imputation, and manual linear regression, and analyzes feature relationships with a correlation matrix. It generates plots showing actual versus predicted ages and outputs a comparison table of average age and mean absolute error (MAE) for each method. The results help evaluate which imputation method best estimates missing ages.  
  
ErrorMetrics: This Python code analyzes the performance of a predictive model by comparing actual and predicted values. It calculates key error metrics—mean absolute error (MAE), mean squared error (MSE), and R²—both manually using NumPy and by validating with scikit-learn functions. The code also displays a table of actual, predicted, and residual values, and generates two visualizations: a predicted vs. actual scatter plot with square markers and a residuals plot with a twist of pink star markers. The worst prediction is highlighted in red in both plots, and the residuals plot uses a custom font for the title and axis labels. These metrics and plots allow the user to evaluate model accuracy, identify the largest errors, and visually assess patterns in the predictions.  

ElementClustering: This model explores patterns in Group 1 (alkali metal) elements by reading their atomic data from a CSV file and applying K-Means clustering based on atomic radius and first ionization energy. The data is first printed to the screen so the user can confirm it loaded correctly, then the user chooses how many clusters to create. The results are shown in a matplotlib scatter plot where each element is colored by its cluster, labeled with its chemical symbol, and the cluster centers are marked with small green Xs. Personalized changes include adding meaningful cluster names (“Lighter Alkali Metals” and “Heavier Alkali Metals”) when using two clusters and adjusting the plot styling to make the visualization clearer and easier to interpret. 
  
GradientDescent: This project generates noisy linear data and fits both a linear regression and a quadratic polynomial regression to it. The program visualizes the noisy data, the true line, and both fitted models, allowing users to manually enter coefficients to calculate the mean squared error (MSE). Two separate loss landscapes are generated: one for the linear model (slope vs intercept) and one for the polynomial model (quadratic vs linear coefficient), showing visually where low-error regions occur in yellow and high-error regions in purple. The results demonstrate how the models approximate how the error changes across different parameter choices, providing insight into model fitting and optimization.  
